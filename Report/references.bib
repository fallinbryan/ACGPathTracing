@article{doi:10.1177/1046878119831420,
author = {Federica Pallavicini and Alessandro Pepe and Maria Eleonora Minissi},
title ={Gaming in Virtual Reality: What Changes in Terms of Usability, Emotional Response and Sense of Presence Compared to Non-Immersive Video Games?},
journal = {Simulation \& Gaming},
volume = {50},
number = {2},
pages = {136-159},
year = {2019},
doi = {10.1177/1046878119831420},
URL = {https://doi.org/10.1177/1046878119831420},
eprint = {https://doi.org/10.1177/1046878119831420},
    abstract = { Background. Virtual reality can provide innovative gaming experiences for present and future game players. However, scientific knowledge is still limited about differences between player’s experience in video games played in immersive modalities and games played in non-immersive modalities (i.e., on a desktop display).Materials and method. Smash Hit was played by 24 young adults in immersive (virtual reality) and non-immersive (desktop) condition. Self-report questionnaires (VAS-A, VAS-HP, VAS-SP, SUS, SUS-II) and psycho-physiological measures (heart rate and skin conductance) were used to assess usability, emotional response and the reported sense of presence.Results. No statistical differences emerged between the immersive and the non-immersive condition regarding usability and performance scores. The general linear model for repeated measures conducted on VAS-A, VAS-HP, VAS-SP scores for the virtual reality condition supported the idea that playing in the immersive display modality was associated with higher self-reported happiness and surprise; analysis on SUS-II revealed that the perceived sense of presence was higher in the virtual reality conditionDiscussion and conclusion. The proposed study provides evidence that (a) playing a video game in virtual reality was not more difficult than playing through a desktop display; (b) players showed a more intense emotional response, as assessed by self-report questionnaires and with psycho-physiological indexes (heart rate and skin conductance), after playing in virtual reality versus after playing through the desktop display; (c) the perceived sense of presence was found to be greater in virtual reality as opposed to the non-immersive condition. }
}



@misc{gao2023nerf,
      title={NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review}, 
      author={Kyle Gao and Yina Gao and Hongjie He and Dening Lu and Linlin Xu and Jonathan Li},
      year={2023},
      eprint={2210.00379},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@InProceedings{Barron_2021_ICCV,
    author    = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
    title     = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {5855-5864}
}
@article{mueller2022instant,
    author = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    journal = {ACM Trans. Graph.},
    issue_date = {July 2022},
    volume = {41},
    number = {4},
    month = jul,
    year = {2022},
    pages = {102:1--102:15},
    articleno = {102},
    numpages = {15},
    url = {https://doi.org/10.1145/3528223.3530127},
    doi = {10.1145/3528223.3530127},
    publisher = {ACM},
    address = {New York, NY, USA},
}
@misc{chen2023mobilenerf,
      title={MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures}, 
      author={Zhiqin Chen and Thomas Funkhouser and Peter Hedman and Andrea Tagliasacchi},
      year={2023},
      eprint={2208.00277},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{martinbrualla2021nerf,
      title={NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections}, 
      author={Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
      year={2021},
      eprint={2008.02268},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{rmdg2003,
author = {Roussou, Maria and Drettakis, George},
year = {2003},
month = {12},
pages = {51-60},
title = {Photorealism and Non-Photorealism in Virtual Heritage Representation},
doi = {10.2312/VAST/VAST03/051-060}
}
@InProceedings{Tancik_2022_CVPR,
    author    = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben and Srinivasan, Pratul P. and Barron, Jonathan T. and Kretzschmar, Henrik},
    title     = {Block-NeRF: Scalable Large Scene Neural View Synthesis},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {8248-8258}
}
@misc{park2023camp,
      title={CamP: Camera Preconditioning for Neural Radiance Fields}, 
      author={Keunhong Park and Philipp Henzler and Ben Mildenhall and Jonathan T. Barron and Ricardo Martin-Brualla},
      year={2023},
      eprint={2308.10902},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{mildenHallOGNerf,
author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
title = {NeRF: representing scenes as neural radiance fields for view synthesis},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3503250},
doi = {10.1145/3503250},
abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
journal = {Commun. ACM},
month = {dec},
pages = {99–106},
numpages = {8}
}
@misc{verbin2021refnerf,
      title={Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields}, 
      author={Dor Verbin and Peter Hedman and Ben Mildenhall and Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
      year={2021},
      eprint={2112.03907},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{mildenhall2021nerf,
      title={NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images}, 
      author={Ben Mildenhall and Peter Hedman and Ricardo Martin-Brualla and Pratul Srinivasan and Jonathan T. Barron},
      year={2021},
      eprint={2111.13679},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{hedman2021snerg,
    title={Baking Neural Radiance Fields for
           Real-Time View Synthesis},
    author={Peter Hedman and Pratul P. Srinivasan and
            Ben Mildenhall and Jonathan T. Barron and
            Paul Debevec},
    journal={ICCV},
    year={2021}
}
@INPROCEEDINGS{10473990,
  author={Ma, Zihang and Li, Zeyu and Wang, Yuanfang and Li, Yu and Yu, Jun and Wang, Kun},
  booktitle={2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)}, 
  title={Booth-NeRF: An FPGA Accelerator for Instant-NGP Inference with Novel Booth-Multiplier}, 
  year={2024},
  volume={},
  number={},
  pages={527-532},
  keywords={Three-dimensional displays;Power demand;Instruction sets;Graphics processing units;Computer architecture;Approximation algorithms;Inference algorithms;FPGA;Hardware Accelerator;NeRF},
  doi={10.1109/ASP-DAC58780.2024.10473990}}

@misc{yuan2022nerfediting,
      title={NeRF-Editing: Geometry Editing of Neural Radiance Fields}, 
      author={Yu-Jie Yuan and Yang-Tian Sun and Yu-Kun Lai and Yuewen Ma and Rongfei Jia and Lin Gao},
      year={2022},
      eprint={2205.04978},
      archivePrefix={arXiv},
      primaryClass={cs.GR}
}
@inproceedings{lino:hal-00535865,
  TITLE = {{A Real-time Cinematography System for Interactive 3D Environments}},
  AUTHOR = {Lino, Christophe and Christie, Marc and Lamarche, Fabrice and Guy, Schofield and Olivier, Patrick},
  URL = {https://hal.science/hal-00535865},
  BOOKTITLE = {{SCA '10 Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation}},
  ADDRESS = {Madrid, Spain},
  PAGES = {139-148},
  YEAR = {2010},
  MONTH = Jul,
  DOI = {10.2312/SCA/SCA10/139-148},
  KEYWORDS = {Cinematography ; Virtual camera control ; Viewpoint computation ; Virtual Camera Planning ; Film Editing},
  PDF = {https://hal.science/hal-00535865/file/2010-AReal-timeCinematographySystemforInteractive3DEnvironments-SCA.pdf},
  HAL_ID = {hal-00535865},
  HAL_VERSION = {v1},
}
@InProceedings{Barron_2022_CVPR,
    author    = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
    title     = {Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {5470-5479}
}

@misc{tiny-cuda-nn,
	author = {M\"uller, Thomas},
	license = {BSD-3-Clause},
	month = {4},
	title = {{tiny-cuda-nn}},
	url = {https://github.com/NVlabs/tiny-cuda-nn},
	version = {1.7},
	year = {2021}
}

@manual{OptixManual,
  title        = {NVIDIA Optix User's Manual},
  author       = {{NVIDIA Corporation}},
  year         = {2021},
  url          = {https://raytracing-docs.nvidia.com/optix8/guide/index.html#preface},
  note         = {Accessed: 2024-05-01}
}

@book{Pharr2016PBR,
  title={Physically Based Rendering: From Theory to Implementation},
  author={Pharr, Matt and Humphreys, Greg},
  edition={3},
  year={2016},
  publisher={Morgan Kaufmann},
  url={https://www.pbrt.org}
}


@article{schoenberger2016sfm,
  title={Structure-from-Motion Revisited},
  author={Schoenberger, Johannes L and Frahm, Jan-Michael},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}


@book{gonzalez2002digital,
  title={Digital Image Processing},
  author={Gonzalez, Rafael C. and Woods, Richard E.},
  year={2002},
  publisher={Prentice Hall}
}
